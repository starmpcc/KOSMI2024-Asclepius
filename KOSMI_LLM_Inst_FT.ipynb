{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KOSMI 2024 Winter School\n",
        "## Session 4. 나만의 거대언어모델 만들어 보기\n",
        "\n",
        "- 연자: 김준우(kjune0322@kaist.ac.kr), 권순준(sean0042@kaist.ac.kr)\n",
        "- 발표자료: https://github.com/starmpcc/KOSMI2024-Asclepius\n",
        "\n"
      ],
      "metadata": {
        "id": "R9cvXkGIWXj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install -q accelerate==0.25.0 peft==0.6.2 bitsandbytes==0.41.1 transformers==4.36.2 trl==0.7.4 einops gradio"
      ],
      "metadata": {
        "id": "dY38frHA5rk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyq7qXBK5hrq"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To save time, first download model and data\n",
        "\n",
        "# Define Quantization Config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "# Load Model and Dataset\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    revision=\"refs/pr/23\"\n",
        ")\n",
        "dataset = load_dataset(\"starmpcc/Asclepius-Synthetic-Clinical-Notes\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_sight = \"right\""
      ],
      "metadata": {
        "id": "Xbz2wAkMV3TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's pre-process dataset\n",
        "\n",
        "# First of all, we have to check how the dataset is composed\n",
        "print(dataset['train'])\n",
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "JJxznOdbVdu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We make this dataset to phi-2 compatible\n",
        "# Phi-2 instruction-answer format: \"Instruct: <prompt>\\nOutput:\"\n",
        "\n",
        "# Make your own prompt!\n",
        "prompt_template=\"\"\"Instruct: Please write down your own prompt.\n",
        "For instance, you can insert the note as {{note}}\n",
        "{note}\n",
        "Model should answer to {{question}} based on the note.\n",
        "{question}\n",
        "You should maintain the phi-2 format\n",
        "Accordingly, the last line must be like the below.\n",
        "Do not forget to insert a new line between your prompt and 'Output'!\n",
        "Output: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# Should get Dict[List] as input, return list of prompts\n",
        "def format_dataset(samples):\n",
        "    outputs = []\n",
        "    for _, note, question, answer, _ in zip(*samples.values()):\n",
        "        out = prompt_template.format(note=note, question=question, answer=answer)\n",
        "        outputs.append(out)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "print(format_dataset({k: [v] for k, v in dataset['train'][0].items()})[0])\n",
        "\n",
        "# Sanity Check\n",
        "prompt_len = len(tokenizer.encode(prompt_template))\n",
        "if prompt_len > 180:\n",
        "    raise ValueError(f\"Your prompt is too long! Please reduce the length from {prompt_len} to 180 tokens\")"
      ],
      "metadata": {
        "id": "mNL48UB8XubL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, let's define dataset.\n",
        "response_template = \"Output:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "8bXTfKImbvEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Due to the resource, we would train the model with short inputs (total length<=512)\n",
        "\n",
        "def prompt_shorther_than(samples, cutoff):\n",
        "    # unpacked = [dict(zip(samples.keys(), v)) for v in zip(*samples)]\n",
        "    prompts = format_dataset(samples)\n",
        "    return [len(i)<=cutoff for i in tokenizer(prompts)['input_ids']]\n",
        "\n",
        "# Filtering all dataset with tokenizer is slow. Preliminary filter them with string length (1 token ~ 4 char)\n",
        "preliminary_filtered = dataset['train'].filter(lambda x: [len(i)<512 * 4.5 for i in format_dataset(x)], batched=True)\n",
        "\n",
        "# Get prompts which is shorter than 512 tokens\n",
        "train_dataset = preliminary_filtered.filter(lambda x: prompt_shorther_than(x, 512), batched=True)\n",
        "\n",
        "sampled_train_dataset = train_dataset.select(range(2000))"
      ],
      "metadata": {
        "id": "rzzF5WGPcIsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFTTrainer Do everything else for you!\n",
        "\n",
        "lora_config=LoraConfig(\n",
        "    r=4,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules= [\"Wqkv\", \"fc1\", \"fc2\" ]\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_strategy=\"no\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=sampled_train_dataset,\n",
        "    formatting_func=format_dataset,\n",
        "    data_collator=collator,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "Y8jY6PNfa8Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "n_Ngq49_BA2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap-up Training\n",
        "model = trainer.model\n",
        "model.eval()\n",
        "\n",
        "note_samples = train_dataset.select(range(len(train_dataset)-10, len(train_dataset)))['note']\n",
        "\n",
        "def inference(note, question, model):\n",
        "    prompt = prompt_template.format(note=note, question=question, answer=\"\")\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
        "    outs = model.generate(input_ids=tokens,\n",
        "                          max_length=512,\n",
        "                          use_cache=True,\n",
        "                          temperature=0.,\n",
        "                          eos_token_id=tokenizer.eos_token_id\n",
        "                          )\n",
        "    output_text = tokenizer.decode(outs.to('cpu')[0], skip_special_tokens=True)\n",
        "    return output_text[len(prompt):]\n",
        "\n",
        "\n",
        "def compare_models(note, question):\n",
        "    with torch.no_grad():\n",
        "        asc_answer = inference(note, question, trainer.model)\n",
        "        with model.disable_adapter():\n",
        "            phi_answer = inference(note, question, trainer.model)\n",
        "    return asc_answer, phi_answer\n",
        "\n",
        "demo = gr.Interface(fn=compare_models, inputs=[gr.Dropdown(note_samples), \"text\"], outputs=[gr.Textbox(label=\"Asclepius\"), gr.Textbox(label=\"Phi-2\")])\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "c_ntjMAe6Uoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}